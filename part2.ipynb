{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caleb Klinger\n",
    "### Grand Canyon University\n",
    "### CST-440\n",
    "### Fabio Marcos De Abreu Santos\n",
    "### Mar 10, 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub Pull Request Miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This Python application mines a GitHub repository to retrieve information on closed pull requests (PRs), including PR number, issue number, file names attached to the PR, date committed, author, title, body, and comments. It also attempts to predict who will close a PR based on the assumption that the last commenter is the person who closes it. The data is saved into a CSV file for further analysis.\n",
    "Prerequisites\n",
    "\n",
    "- Python 3.x installed on your machine\n",
    "- pip for installing Python packages\n",
    "- A GitHub account and a Personal Access Token (PAT) with at least repo scope permissions for accessing the GitHub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Setup logging configuration\n",
    "logging.basicConfig(filename='app_activity.log',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "logging.info('Application start')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "\n",
    "# GitHub API setup\n",
    "GITHUB_TOKEN = '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'\n",
    "GITHUB_REPO = 'lencx/ChatGPT'  # Format: 'owner/repository_name'\n",
    "HEADERS = {'Authorization': f'token {GITHUB_TOKEN}'}\n",
    "PR_ENDPOINT = f'https://api.github.com/repos/{GITHUB_REPO}/pulls'\n",
    "PARAMS = {'state': 'closed', 'per_page': 100}  # Fetch 100 PRs per API call\n",
    "\n",
    "def fetch_prs():\n",
    "    prs = []\n",
    "    page = 1\n",
    "    while len(prs) < 1000:\n",
    "        logging.info(f'Fetching PRs page {page}')\n",
    "        response = requests.get(PR_ENDPOINT, headers=HEADERS, params={**PARAMS, 'page': page})\n",
    "        if response.status_code != 200:\n",
    "            logging.error(f'Failed to fetch PRs, status code: {response.status_code}')\n",
    "            break  # Stop if we encounter an error\n",
    "        batch = response.json()\n",
    "        if not batch:\n",
    "            logging.info('No more PRs to fetch')\n",
    "            break  # Stop if there are no more PRs to fetch\n",
    "        prs.extend(batch)\n",
    "        logging.debug(f'Fetched {len(batch)} PRs')\n",
    "        page += 1\n",
    "    return prs[:1000]  # Return the first 1000 PRs\n",
    "\n",
    "def fetch_pr_details(pr):\n",
    "    logging.debug(f'Fetching details for PR #{pr[\"number\"]}')\n",
    "    # Fetch additional details like comments and attached files\n",
    "    files_response = requests.get(pr['url'] + '/files', headers=HEADERS)\n",
    "    comments_response = requests.get(pr['url'] + '/comments', headers=HEADERS)\n",
    "    \n",
    "    files = [file['filename'] for file in files_response.json()]\n",
    "    comments = [comment['body'] for comment in comments_response.json()]\n",
    "    closed_at = pr.get('closed_at', 'Not Closed')  # Get 'closed_at', or use 'Not Closed' if none\n",
    "    \n",
    "    return {\n",
    "        'PR Number': pr['number'],\n",
    "        'Issue Number': pr['head']['ref'],  # This might not accurately reflect the issue number; adjust as needed.\n",
    "        'Files': ', '.join(files),\n",
    "        'Date Committed': pr['created_at'],\n",
    "        'Date Closed': closed_at,  # Include the 'Date Closed' information\n",
    "        'Author': pr['user']['login'],\n",
    "        'Title': pr['title'],\n",
    "        'Body': pr['body'],\n",
    "        'Comments': ' | '.join(comments)  # Concatenate comments, separated by a pipe\n",
    "    }\n",
    "\n",
    "\n",
    "def save_to_csv(pr_details):\n",
    "    keys = pr_details[0].keys()\n",
    "    with open('pr_details_with_last_commenter.csv', 'w', newline='') as file:\n",
    "        dict_writer = csv.DictWriter(file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(pr_details)\n",
    "\n",
    "def main():\n",
    "    prs = fetch_prs()\n",
    "    pr_details = [fetch_pr_details(pr) for pr in prs]\n",
    "    save_to_csv(pr_details)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Application shutdown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def load_csv_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "def find_most_common_author(data):\n",
    "    authors_count = {}\n",
    "    for row in data:\n",
    "        author = row['Author']\n",
    "        if author in authors_count:\n",
    "            authors_count[author] += 1\n",
    "        else:\n",
    "            authors_count[author] = 1\n",
    "    \n",
    "    most_common_author = max(authors_count, key=authors_count.get)\n",
    "    return most_common_author, authors_count[most_common_author]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_time_to_close(data):\n",
    "    total_time = 0\n",
    "    count = 0\n",
    "    for row in data:\n",
    "        try:\n",
    "            created_at = datetime.strptime(row['Date Committed'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            closed_at = datetime.strptime(row['Date Closed'], \"%Y-%m-%dT%H:%M:%SZ\")  # Adjusted to the correct column name\n",
    "            total_time += (closed_at - created_at).total_seconds()\n",
    "            count += 1\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing data for row: {e}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing dates for row: {e}\")\n",
    "\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    average_time_seconds = total_time / count\n",
    "    return average_time_seconds / 3600  # Convert to hours for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12: 50 PRs\n",
      "2023-01: 31 PRs\n",
      "2023-02: 30 PRs\n",
      "2023-03: 11 PRs\n",
      "2023-04: 3 PRs\n",
      "2023-05: 3 PRs\n",
      "2023-06: 1 PRs\n",
      "2023-07: 4 PRs\n",
      "2023-09: 1 PRs\n",
      "2023-10: 2 PRs\n",
      "2023-12: 1 PRs\n",
      "2024-01: 1 PRs\n",
      "2024-03: 2 PRs\n",
      "Most common PR author: lencx with 2 PRs\n",
      "Average time to close a PR: 79.61062698412698 hours\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/lettuce/SchoolCode/Analytics for Dynamic Social Network (440)/week 12/pr_details_with_last_commenter.csv'\n",
    "data = load_csv_data(file_path)\n",
    "most_common_author, count = find_most_common_author(data)\n",
    "average_time_to_close = calculate_average_time_to_close(data)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def pr_distribution_over_time(data):\n",
    "    monthly_pr_count = defaultdict(int)\n",
    "    for row in data:\n",
    "        date = datetime.strptime(row['Date Committed'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        month_key = date.strftime('%Y-%m')  # Group by year and month\n",
    "        monthly_pr_count[month_key] += 1\n",
    "    return dict(monthly_pr_count)\n",
    "\n",
    "# Assuming you've already loaded the data with load_csv_data\n",
    "pr_monthly_distribution = pr_distribution_over_time(data)\n",
    "for month, count in sorted(pr_monthly_distribution.items()):\n",
    "    print(f\"{month}: {count} PRs\")\n",
    "\n",
    "\n",
    "print(f\"Most common PR author: {most_common_author} with {count} PRs\")\n",
    "print(f\"Average time to close a PR: {average_time_to_close} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "https://github.com/lencx/ChatGPT \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
